{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Data Cleaning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisowen6/Text_Data_Cleaning/blob/master/Text_Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji-iLg4SYC4h",
        "colab_type": "text"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DwK6qqNfhPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f8aa6e7a-6d78-4b0a-b1a3-cf04c725b21e"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPeKO4i2YFjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0c352652-3092-4901-a2a5-74323badbbc8"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "from bs4 import BeautifulSoup\n",
        "import inflect\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import contractions\n",
        "import string"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoaOPHSlX8Kc",
        "colab_type": "text"
      },
      "source": [
        "# Mount the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTnUPHtOX3ww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e95d3457-6f38-4d15-aada-d4a5c11f1095"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0ewZWfJYAKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngENUGqvYBXR",
        "colab_type": "text"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTCOOds2a6s8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29809878-a125-41d0-ee65-e2570ffab04b"
      },
      "source": [
        "link ='https://drive.google.com/open?id=1szYV-zxOuHUWf2RlIKuvZD3qLccRmKO-'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Microblog_Train_twitter_full.json')  \n",
        "with open('Microblog_Train_twitter_full.json') as train_file:\n",
        "    dict_train = json.load(train_file)\n",
        "microblog_train_twitter_full = pd.DataFrame.from_dict(dict_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1szYV-zxOuHUWf2RlIKuvZD3qLccRmKO-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXXGb30a64z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b2110ff-8127-4f6c-b294-eb2269c1ff62"
      },
      "source": [
        "link ='https://drive.google.com/open?id=1hsmjwP7FiDAkq4hgWbjVTUJ9fY1yKEyc'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Microblog_Train_stocktwits_full.json')  \n",
        "with open('Microblog_Train_stocktwits_full.json') as train_file:\n",
        "    dict_train = json.load(train_file)\n",
        "microblog_train_stocktwits_full = pd.DataFrame.from_dict(dict_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1hsmjwP7FiDAkq4hgWbjVTUJ9fY1yKEyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA1gPU2TfMl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71072a20-de42-4a77-a2fb-ad97cf88cfd3"
      },
      "source": [
        "#Abbreviation & Slang Dict\n",
        "link ='https://drive.google.com/open?id=1ptzEljjFTQNF3FRR9wO9G7D0I9DWwjQR'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('abbreviation_dict.txt')  \n",
        "abb_df = pd.read_table('abbreviation_dict.txt', sep='\\s+', names=('Abbreviation', 'Normal'))\n",
        "abb=pd.Series(abb_df['Normal'])\n",
        "abb.index=abb_df['Abbreviation']\n",
        "abb_dict=dict(abb)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1ptzEljjFTQNF3FRR9wO9G7D0I9DWwjQR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbFVggCfa_ue",
        "colab_type": "text"
      },
      "source": [
        "# Rebuild Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHab_-NYbC_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_df=microblog_train_twitter_full[['cashtag','sentiment score','spans','text','created_at']].copy()\n",
        "twitter_df['source']=['twitter' for x in range(len(twitter_df))] #add source column\n",
        "twitter_df['created_at']=pd.to_datetime(twitter_df['created_at']).apply(lambda x: '[0am,9am)' \n",
        "                                                                        if (x.hour>=0 and x.hour<9) else '[9am,3pm)' \n",
        "                                                                        if (x.hour>=9 and x.hour<15) else '[3pm,24pm)')  #encoding time created"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWf48AehbHmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stocktwits_df=microblog_train_stocktwits_full[['cashtag','sentiment score','spans']].copy()\n",
        "\n",
        "stocktwits_message= microblog_train_stocktwits_full['message'].apply(pd.Series) #message data\n",
        "stocktwits_user=stocktwits_message['user'].apply(pd.Series) #user data\n",
        "\n",
        "stocktwits_df['text']=stocktwits_message['body']\n",
        "stocktwits_df['created_at']=stocktwits_message['created_at']\n",
        "stocktwits_df['source']=microblog_train_stocktwits_full['source']\n",
        "stocktwits_df['official_account']=stocktwits_user['official']\n",
        "stocktwits_df['sentiment']=stocktwits_message['entities'].apply(pd.Series)['sentiment'].apply(lambda x : x['basic'] if (type(x) is dict) else np.nan) #add sentiment: 'bearish' or 'bullish'\n",
        "stocktwits_df['liked_by_self']=stocktwits_message['liked_by_self']\n",
        "stocktwits_df['conversation_parent']=stocktwits_message['conversation'].apply(lambda x : x['parent'] if (type(x) is dict) else np.nan)\n",
        "stocktwits_df['conversation_replies']=stocktwits_message['conversation'].apply(lambda x : x['replies'] if (type(x) is dict and not math.isnan(x['replies'])) else 0)\n",
        "stocktwits_df['conversation_replies']=stocktwits_df['conversation_replies'].apply(lambda x: (x-min(stocktwits_df['conversation_replies']))/(max(stocktwits_df['conversation_replies'])-min(stocktwits_df['conversation_replies']))) #min-max scaling\n",
        "stocktwits_df['total_likes']=stocktwits_message['likes'].apply(lambda x : x['total'] if (type(x) is dict and not math.isnan(x['total'])) else 0)\n",
        "stocktwits_df['total_likes']=stocktwits_df['total_likes'].apply(lambda x: (x-min(stocktwits_df['total_likes']))/(max(stocktwits_df['total_likes'])-min(stocktwits_df['total_likes']))) #min-max scaling\n",
        "stocktwits_df['created_at']=pd.to_datetime(stocktwits_df['created_at']).apply(lambda x: '[0am,9am)' \n",
        "                                                                              if (x.hour>=0 and x.hour<9) else '[9am,3pm)' \n",
        "                                                                              if (x.hour>=9 and x.hour<15) else '[3pm,24pm)') #encoding time created"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he182w3TbJpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to numeric\n",
        "twitter_df['sentiment score']=pd.to_numeric(twitter_df['sentiment score'])\n",
        "stocktwits_df['sentiment score']=pd.to_numeric(stocktwits_df['sentiment score'])\n",
        "\n",
        "#One hot encoding\n",
        "stocktwits_df['official_account']=stocktwits_df['official_account'].apply(lambda x: 1 if x==True else 0 if x==False else x)\n",
        "stocktwits_df['liked_by_self']=stocktwits_df['liked_by_self'].apply(lambda x: 1 if x==True else 0 if x==False else x)\n",
        "stocktwits_df['conversation_parent']=stocktwits_df['conversation_parent'].apply(lambda x: 1 if x==True else 0 if x==False else x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv1tJI2ebLcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Concatenate both dataframe\n",
        "concat_df=pd.concat([stocktwits_df,twitter_df])\n",
        "concat_df=concat_df[pd.isnull(concat_df.text)==False]\n",
        "concat_df=concat_df.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCWkMh3tbZYQ",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3e99U81bbD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inspired by: \n",
        "# https://www.dotnetperls.com/punctuation-python\n",
        "# https://github.com/tthustla/twitter_sentiment_analysis_part1/blob/master/Capstone_part2.ipynb\n",
        "# https://github.com/Deffro/text-preprocessing-techniques/blob/master/techniques.py\n",
        "\n",
        "#Word ordinal encoding\n",
        "p = inflect.engine()\n",
        "word_to_number_mapping = {}\n",
        "for i in range(1, 2000):\n",
        "  word_form = p.number_to_words(i)  # 1 -> 'one'\n",
        "  ordinal_word = p.ordinal(word_form)  # 'one' -> 'first'\n",
        "  ordinal_number = p.ordinal(i)  # 1 -> '1st'\n",
        "  word_to_number_mapping[ordinal_word] = ordinal_number  # 'first': '1st'\n",
        "  \n",
        "  \n",
        "def elongated_word(word):\n",
        "    \"\"\"\n",
        "     Replaces an elongated word with its basic form, unless the word exists in the lexicon \n",
        "     \"\"\"\n",
        "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    repl = r'\\1\\2\\3'\n",
        "    if (len(word)>2 and word[0] != '$'):#if not Stock Market symbol\n",
        "      if wn.synsets(word):\n",
        "          return word\n",
        "      repl_word = repeat_regexp.sub(repl, word)\n",
        "      if repl_word != word:      \n",
        "          return elongated_word(repl_word)\n",
        "      else:       \n",
        "          return repl_word\n",
        "    else:\n",
        "      return word\n",
        "    \n",
        "def isfloat(value):\n",
        "  ''' \n",
        "  Check if value is float or not\n",
        "  '''\n",
        "  try:\n",
        "    float(value)\n",
        "    return True\n",
        "  except ValueError:\n",
        "    return False\n",
        "\n",
        "def tokenize(sentence):\n",
        "  '''\n",
        "  tokenize input sentence into token\n",
        "  '''\n",
        "  token_list=nltk.regexp_tokenize(sentence, pattern=r\"\\s|[\\.,;]\\D\", gaps=True)\n",
        "  return(token_list)\n",
        "\n",
        "def sentences_cleaner(sentence):\n",
        "  '''\n",
        "  clean input sentence  \n",
        "  '''\n",
        "  try:\n",
        "    mention_pat= r'@[A-Za-z0-9_]+'\n",
        "    mention_2_pat=r'@[A-Za-z0-9_]+:\\s'\n",
        "    retweet_pat=r'^RT +'\n",
        "    dollars_pat=r'\\$ +'\n",
        "    http_pat = r'https?://[^ ]+'\n",
        "    www_pat = r'www.[^ ]+'\n",
        "    apos_pat=r'\"+|\"$|\"+\"$'\n",
        "      \n",
        "    soup = BeautifulSoup(sentence, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    \n",
        "    #HTML decoding remove BOM\n",
        "    try:\n",
        "      bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "      bom_removed = souped\n",
        "          \n",
        "    #Delete mention\n",
        "    stripped = re.sub(mention_2_pat,\"\", bom_removed)\n",
        "    stripped = re.sub(mention_pat,\"\", stripped)\n",
        "      \n",
        "    #Delete retweet\n",
        "    stripped=re.sub(retweet_pat,\"\",stripped)\n",
        "      \n",
        "    #Transform any url into '_url'\n",
        "    stripped = re.sub(http_pat, '_url', stripped)\n",
        "    stripped = re.sub(www_pat, '_url', stripped)\n",
        "    \n",
        "    #Transfrom abbreviation & slang word  into normal words based on abb_dict corpus\n",
        "    abbreviation_handled=' '.join(pd.Series(stripped.split()).apply(lambda x: abb_dict[x] if x in abb_dict.keys() else x).to_list())\n",
        "   \n",
        "    #Transform contracted words into normal words\n",
        "    contraction_handled =contractions.fix(abbreviation_handled)\n",
        "      \n",
        "    #Join the stock symbol\n",
        "    dollars_handled=re.sub(dollars_pat,'$',contraction_handled)\n",
        "      \n",
        "    #Transform elongated words into normal words\n",
        "    elongated_handled=' '.join(pd.Series(dollars_handled.split()).apply(lambda x: elongated_word(x[:-1])+x[-1] if (x[-1] in string.punctuation and not isfloat(x)) else elongated_word(x) if not isfloat(x) else x))\n",
        "      \n",
        "    #Transform ordinal number\n",
        "    ordinal_handled=' '.join(pd.Series(elongated_handled.split()).apply(lambda x: word_to_number_mapping[x.lower()] if x.lower() in word_to_number_mapping.keys() else x))\n",
        "      \n",
        "    #Remove unnecesary apostrophes \n",
        "    apos_handled=re.sub(apos_pat,'',ordinal_handled)\n",
        "      \n",
        "    #Split Last Word Punctuation\n",
        "    wordpunct=wordpunct_tokenize(apos_handled)\n",
        "    if (len(wordpunct[-1])>1 and wordpunct[-1][-1] in string.punctuation and wordpunct[-2] not in string.punctuation) or (wordpunct[-1] in string.punctuation and wordpunct[-2] not in string.punctuation):\n",
        "      words =tokenize(apos_handled)\n",
        "      words[-1]=wordpunct[-2]\n",
        "      words.append(wordpunct[-1])\n",
        "    else:\n",
        "      words =tokenize(apos_handled)\n",
        "\n",
        "    return (\" \".join(words)).strip()\n",
        "  except:\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPsfptTmbdA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e1617a34-48a2-44be-b7cd-a55e81ed4181"
      },
      "source": [
        "print('BEFORE CLEANING: {}'.format(\"RT $  FB TOMORROW IS MY FIRST BIG WINNNN!!!! #XOXO Trust me it will go up 50.78%!!! #MYWIN\"))\n",
        "print()\n",
        "print('AFTER CLEANING: {}'.format(sentences_cleaner(\"RT $  FB TOMORROW IS MY FIRST BIG WINNNN!!!! #XOXO Trust me it will go up 50.78%!!! #MYWIN\")))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BEFORE CLEANING: RT $  FB TOMORROW IS MY FIRST BIG WINNNN!!!! #XOXO Trust me it will go up 50.78%!!! #MYWIN\n",
            "\n",
            "AFTER CLEANING: $FB TOMORROW IS MY 1st BIG WIN!!!! #XOXO Trust me it will go up 50.78%!!! #MYWIN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kML4Cjxbfb1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e68c2a39-2fad-4089-d8da-6d3d77c937db"
      },
      "source": [
        "print('BEFORE CLEANING: {}'.format(concat_df.loc[81,'text']))\n",
        "print()\n",
        "print('AFTER CLEANING: {}'.format(sentences_cleaner(concat_df.loc[81,'text'])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BEFORE CLEANING: RT @Montisumo Down day for all biotechs Good to load up Be confident. $ONTY $CHTP $AIS $HGSI $DNDN &gt;&gt; Agreed $IBB $XBI all same downness\n",
            "\n",
            "AFTER CLEANING: Down day for all biotechs Good to load up Be confident $ONTY $CHTP $AIS $HGSI $DNDN >> Agreed $IBB $XBI all same downes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KEHH2BnbhRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "40a8c51e-9cbd-4dd9-f8ba-ad377610256b"
      },
      "source": [
        "print('BEFORE CLEANING: {}'.format(stocktwits_df.loc[43,'text']))\n",
        "print()\n",
        "print('AFTER CLEANING: {}'.format(sentences_cleaner(stocktwits_df.loc[43,'text'])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BEFORE CLEANING: @joekidd lost 600 in like 3 days for my $C investment. Sure wish bad news came out just a few days earlier. Think I&#39;ll go over to Russia!\n",
            "\n",
            "AFTER CLEANING: lost 600 in like 3 days for my $C investment Sure wish bad news came out just a few days earlier Think I will go over to Russia !\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}